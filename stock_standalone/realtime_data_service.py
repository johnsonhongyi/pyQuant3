# -*- coding:utf-8 -*-
import time
import threading
import pandas as pd
from collections import deque, defaultdict
from typing import Callable, Any, Dict, List, Union, Optional
from JohnsonUtil import LoggerFactory
from JohnsonUtil import commonTips  as cct
import psutil
import os
import sqlite3

logger = LoggerFactory.getLogger()

# Lightweight K-line item using __slots__ to save memory
class KLineItem:
    __slots__ = ('time', 'open', 'high', 'low', 'close', 'volume', 'cum_vol_start')
    def __init__(self, time: int, open: float, high: float, low: float, close: float, volume: float, cum_vol_start: float):
        self.time = time
        self.open = open
        self.high = high
        self.low = low
        self.close = close
        self.volume = volume
        self.cum_vol_start = cum_vol_start
    
    def as_dict(self) -> dict:
        return {k: getattr(self, k) for k in self.__slots__}

class MinuteKlineCache:
    """
    åˆ†æ—¶Kçº¿ç¼“å­˜
    æ¯è‚¡ä¿ç•™æœ€è¿‘ N æ ¹ 1åˆ†é’ŸKçº¿
    """
    def __init__(self, max_len: int = 240):
        self.max_len = max_len
        # {code: deque([KLineItem, ...])}
        self.cache: dict[str, deque] = {}
        self.last_update_ts: dict[str, float] = {}

    def set_mode(self, max_len: int):
        """åŠ¨æ€åˆ‡æ¢å›æº¯æ—¶é•¿ï¼šä¸æ¸…é™¤æ•°æ®ï¼Œä»…è£å‰ªæ—§èŠ‚ç‚¹ä»¥å›æ”¶å†…å­˜"""
        if self.max_len != max_len:
            logger.info(f"âœ‚ï¸ MinuteKlineCache Trimming: {self.max_len} -> {max_len} nodes")
            self.max_len = max_len
            # å¯¹æ‰€æœ‰ç°æœ‰æ•°æ®è¿›è¡Œé‡å»ºä»¥åŒæ­¥ maxlen å±æ€§
            for code in list(self.cache.keys()):
                dq = self.cache[code]
                # æ— è®ºå½“å‰é•¿åº¦å¦‚ä½•ï¼Œéƒ½å¿…é¡»é‡å»º deque ä»¥ä¿®æ”¹åªè¯»çš„ maxlen å±æ€§
                self.cache[code] = deque(list(dq)[-max_len:], maxlen=max_len)

    def clear(self):
        """å®Œå…¨æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.last_update_ts.clear()

    def get_klines(self, code: str, n: int = 60) -> list:
        if code not in self.cache:
            return []
        nodes = list(self.cache[code])[-n:]
        # Support dict-based access for existing strategy code
        return [node.as_dict() for node in nodes]

    def update(self, code: str, tick: dict):
        """
        ä½¿ç”¨å®æ—¶ Tick æ›´æ–° K çº¿ (ä¿ç•™å•æ¡æ›´æ–°æ¥å£ç”¨äºå…¼å®¹æ€§)
        """
        try:
            price = float(tick.get('trade', 0) or tick.get('now', 0))
            if price <= 0: return

            ts = int(tick.get('timestamp') or time.time())
            minute_ts = ts - (ts % 60)
            
            # ä½¿ç”¨æ›´è½»é‡ã€æ— å‡½æ•°è°ƒç”¨å¼€é”€çš„å†…éƒ¨é€»è¾‘
            self._update_internal(code, price, float(tick.get('volume', 0)), minute_ts)
        except Exception as e:
            logger.error(f"MinuteKlineCache update error: {e}")

    def batch_update(self, df: pd.DataFrame, subscribers: set):
        """
        æ‰¹é‡æ›´æ–° K çº¿ (çŸ¢é‡åŒ–/é«˜æ€§èƒ½æ¨¡å¼)
        """
        try:
            if df.empty: return
            
            # 1. çŸ¢é‡åŒ–è¿‡æ»¤ï¼šä»…å¤„ç†æ„Ÿå…´è¶£çš„è‚¡ç¥¨
            # è¿™æ ·åšå¯ä»¥é¿å…å¯¹å…¨å¸‚åœº 5000+ è‚¡ç¥¨è¿›è¡Œå¾ªç¯
            mask = df['code'].isin(self.cache.keys() | subscribers)
            if not mask.any(): return
            
            active_df = df[mask]
            
            # 2. é¢„è®¡ç®—å…¬å…±ä¿¡æ¯
            # å‡è®¾ä¸€ä¸ª Batch å†…çš„æ—¶é—´æˆ³åŸºæœ¬ä¸€è‡´
            ts = int(active_df['timestamp'].iloc[0] if 'timestamp' in active_df.columns else time.time())
            minute_ts = ts - (ts % 60)
            
            # 3. é«˜é€Ÿè¿­ä»£
            # itertuples æ‰“åŒ…æˆ NamedTupleï¼Œè®¿é—®é€Ÿåº¦è¿œå¿«äº to_dict('records')
            # å¿…é¡»æŒ‡å®šå­—æ®µé¡ºåºæˆ–ç›´æ¥æŒ‰ä½ç½®è®¿é—®
            for row in active_df.itertuples(index=False):
                # row å¯¹åº” columns: code, trade, volume, high, low, open, amount, (timestamp)
                # æ³¨æ„ï¼šcreate_dummy_data æˆ– fetch_and_process è¿”å›çš„åˆ—é¡ºåºå¯èƒ½ä¸åŒ
                # å»ºè®®é€šè¿‡ getattr è®¿é—®ä»¥ä¿è¯ç¨³å¥æ€§ï¼Œè™½ç„¶ç¨æ…¢ä¸€ç‚¹ç‚¹ä½†æ¯” dict å¿«
                code = row.code
                price = getattr(row, 'trade', 0)
                if price <= 0: continue
                volume = getattr(row, 'volume', getattr(row, 'vol', 0))
                
                self._update_internal(code, price, float(volume), minute_ts)
                
        except Exception as e:
            logger.error(f"MinuteKlineCache batch_update error: {e}")

    def _update_internal(self, code: str, price: float, current_cum_vol: float, minute_ts: int):
        """
        å†…éƒ¨æ ¸å¿ƒæ›´æ–°é€»è¾‘ï¼ˆæœ€å°åŒ–å¼€é”€ï¼‰
        """
        if code not in self.cache:
            self.cache[code] = deque(maxlen=self.max_len)
        klines = self.cache[code]
        
        if not klines:
            klines.append(KLineItem(
                time=minute_ts, open=price, high=price, low=price, close=price,
                volume=0.0, cum_vol_start=current_cum_vol
            ))
        else:
            last_k = klines[-1]
            if last_k.time == minute_ts:
                last_k.high = max(last_k.high, price)
                last_k.low = min(last_k.low, price)
                last_k.close = price
                last_k.volume = current_cum_vol - last_k.cum_vol_start
            else:
                klines.append(KLineItem(
                    time=minute_ts, open=price, high=price, low=price, close=price,
                    volume=0.0, cum_vol_start=current_cum_vol
                ))

    def detect_v_shape(self, code: str, window: int = 30) -> bool:
        """
        æ£€æµ‹ V å‹åè½¬ (30åˆ†é’Ÿçª—å£)
        é€»è¾‘:
        1. çª—å£å†…æœ€ä½ç‚¹è·Œå¹…è¾ƒæ·± (ç›¸å¯¹äºçª—å£èµ·å§‹æˆ–å½“æ—¥å¼€ç›˜, è¿™é‡Œç®€åŒ–ä¸ºç›¸å¯¹äºçª—å£å†…æœ€é«˜ç‚¹è·Œå¹… > 2%)
        2. å½“å‰ä»·æ ¼è¾ƒæœ€ä½ç‚¹æ˜æ˜¾åå¼¹ (åå¼¹å¹…åº¦ > 1.5%)
        3. å½“å‰ä»·æ ¼æ¥è¿‘æˆ–è¶…è¿‡çª—å£èµ·å§‹ä»·
        """
        klines = self.get_klines(code, n=window)
        if len(klines) < 10:
            return False
            
        try:
            closes = [k['close'] for k in klines]
            lows = [k['low'] for k in klines]
            highs = [k['high'] for k in klines]
            
            curr_price = closes[-1]
            min_low = min(lows)
            max_high = max(highs)
            
            # 1. å¹¶æ²¡æœ‰å¤ªå¤§çš„è·Œå¹…ï¼Œå¿½ç•¥
            # (æœ€é«˜ç‚¹ - æœ€ä½ç‚¹) / æœ€é«˜ç‚¹ < 2% -> æ³¢åŠ¨å¤ªå°
            if max_high == 0: return False
            drop_range = (max_high - min_low) / max_high
            if drop_range < 0.02:
                return False
                
            # 2. ä»æœ€ä½ç‚¹åå¼¹åŠ›åº¦
            # (å½“å‰ - æœ€ä½) / æœ€ä½
            if min_low == 0: return False
            rebound = (curr_price - min_low) / min_low
            
            # 3. åå¼¹ç¡®è®¤
            if rebound > 0.015:
                # è¿›ä¸€æ­¥ç¡®è®¤å½¢æ€ï¼šæœ€ä½ç‚¹å‡ºç°åœ¨çª—å£ä¸­é—´è€Œéåˆšå¼€å§‹
                # ç®€å•å¤„ç†ï¼šåªè¦åå¼¹å¤ŸçŒ›ä¸”åˆšè·Œè¿‡
                return True
                
        except Exception as e:
            logger.error(f"V-shape check error: {e}")
            
        return False

class TickAggregator:
    """
    Tick èšåˆå™¨
    ç”¨äºè¿½è¸ª tick çº§åˆ«çš„ä¹°å–ç›˜å£å˜åŒ–
    """
    def __init__(self):
        self.last_ticks = {}
    
    def process(self, code: str, current_tick: dict):
        # ç®€å•æ¯”å¯¹ä¸Šä¸€ç¬” tick è®¡ç®—ä¸»åŠ¨ä¹°å–
        # æš‚æ—¶åªåšå ä½ï¼Œåç»­æ‰©å±• Level2 åˆ†æ
        pass

class IntradayEmotionTracker:
    """
    ç›˜ä¸­æƒ…ç»ªè¿½è¸ªå™¨
    è®¡ç®—ä¸ªè‚¡åŠå¸‚åœºæƒ…ç»ªåˆ†
    """
    def __init__(self):
        self.scores = {} # {code: score}

    def clear(self):
        self.scores.clear()

    def update_batch(self, df: pd.DataFrame):
        """
        æ‰¹é‡æ›´æ–°æƒ…ç»ªåˆ†
        df: åŒ…å« 'percent', 'amount', 'volume' ç­‰åˆ—
        """
        try:
            if df.empty: return
            
            # ç®€å•ç®—æ³•ç¤ºä¾‹ï¼šæ¶¨å¹… + é‡æ¯”è´¡çŒ®
            # å®é™…é€»è¾‘å¯è¿ç§»åŸ Emotion ç®—æ³•
            # è¿™é‡Œä»…åšç®€å•æ˜ å°„ä½œä¸ºå ä½
            if 'percent' in df.columns:
                # å½’ä¸€åŒ– emotion score 0-100
                # å‡è®¾ percent > 9 ä¸º 100, < -9 ä¸º 0
                self.scores = df.set_index('code')['percent'].to_dict()
                
        except Exception as e:
            logger.error(f"IntradayEmotionTracker update error: {e}")

    def get_score(self, code: str) -> float:
        return self.scores.get(code, 50.0) # é»˜è®¤ 50 ä¸­æ€§

try:
    import psutil
except ImportError:
    psutil = None

from scraper_55188 import Scraper55188
from JohnsonUtil import commonTips as cct

class DataPublisher:
    """
    æ•°æ®åˆ†å‘å™¨ (æ ¸å¿ƒå…¥å£)
    """
    def __init__(self, high_performance: bool = False, scraper_interval: int = 600):
        self.paused = False
        self.high_performance = high_performance # HP: ~4.0h, Legacy: ~2.0h (Dynamic nodes)
        self.auto_switch_enabled = True
        self.mem_threshold_mb = 500.0 # é˜ˆå€¼è°ƒä½è‡³ 500MB
        self.node_threshold = 1000000 # é»˜è®¤ 100ä¸‡ä¸ªèŠ‚ç‚¹è§¦å‘é™çº§
        
        # Interval Settings
        self.expected_interval = 60 # é»˜è®¤ 1åˆ†é’Ÿ
        self.last_batch_clock = 0.0
        self.batch_intervals = deque(maxlen=20) # æœ€è¿‘ 20 æ‰¹æ¬¡çš„é—´éš”(ç§’)
        
        # 55188 Scraper Settings
        self.scraper_interval = scraper_interval
        self.current_scraper_wait = scraper_interval
        self.max_scraper_wait = 1800 # æœ€å¤§ 30 åˆ†é’Ÿ
        
        # Sector Persistence Settings
        self.db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "concept_pg_data.db")
        self.sector_cache = {} # {name: score}
        self.last_db_check = 0.0

        # Time-based goals (Hours)
        self.TARGET_HOURS_HP = 4.0
        self.TARGET_HOURS_LEGACY = 2.0

        # Mode-based settings: Calculate max_len based on default 60s first
        default_interval = 60
        cache_len = int((self.TARGET_HOURS_HP * 3600) / default_interval) if high_performance else int((self.TARGET_HOURS_LEGACY * 3600) / default_interval)
        self.kline_cache = MinuteKlineCache(max_len=cache_len)
        
        self.emotion_tracker = IntradayEmotionTracker()
        self.subscribers: Dict[str, List[Callable]] = defaultdict(list)
        
        # Performance Tracking
        self.start_time = time.time()
        self.update_count = 0
        self.total_rows_processed = 0
        self.last_batch_time = 0
        self.max_batch_time = 0.0
        self.batch_rates_dq = deque(maxlen=10) # Last 10 batch rates (rows/sec)
        
        # 55188 External Data Integration
        self.scraper_55188 = Scraper55188()
        self.ext_data_55188 = pd.DataFrame()
        self.last_ext_update_ts = 0.0

        # Start maintenance thread
        self.maintenance_thread = threading.Thread(target=self._maintenance_task, daemon=True)
        self.maintenance_thread.start()
        
        # Start external data scraper thread
        self.scraper_thread = threading.Thread(target=self._scraper_task, daemon=True)
        self.scraper_thread.start()

    def reset_state(self):
        """
        æ¯æ—¥é‡ç½®çŠ¶æ€ï¼ˆæ”¶ç›˜åæˆ–å¼€ç›˜å‰è°ƒç”¨ï¼‰
        æ¸…é™¤æ‰€æœ‰ç§¯å‹çš„ K çº¿å’Œæƒ…ç»ªæ•°æ®ï¼Œé‡Šæ”¾å†…å­˜
        """
        logger.info("ğŸŒ€ RealtimeDataService performing Daily Reset...")
        try:
            self.kline_cache.clear()
            self.emotion_tracker.clear()
            self.subscribers.clear() # Optional: Clear subscribers if connection needs reset
            self.update_count = 0
            self.total_rows_processed = 0
            
            # Re-init performance tracking
            self.max_batch_time = 0.0
            self.batch_rates_dq.clear()
            self.start_time = time.time()
            
            if psutil:
                mem = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
                logger.info(f"âœ… Reset Complete. Current Memory: {mem:.2f} MB")
            else:
                logger.info("âœ… Reset Complete.")
                
        except Exception as e:
            logger.error(f"Reset failed: {e}")

    def set_paused(self, paused: bool):
        """è®¾ç½®æœåŠ¡æš‚åœçŠ¶æ€"""
        self.paused = paused
        logger.info(f"ğŸš¦ RealtimeDataService paused set to: {paused}")

    def is_paused(self) -> bool:
        """è·å–æœåŠ¡æ˜¯å¦æš‚åœ"""
        return self.paused

    def set_expected_interval(self, seconds: int):
        """ç”±å¤–éƒ¨ UI åŒæ­¥å½“å‰æŠ“å–é¢‘ç‡ï¼Œç”¨äºè¾…åŠ©è®¡ç®— Kçº¿æ‰€éœ€æ•°é‡"""
        if seconds > 0 and self.expected_interval != seconds:
            logger.info(f"â±ï¸ DataPublisher expected interval updated: {seconds}s")
            self.expected_interval = seconds
            # ç«‹å³è§¦å‘ä¸€æ¬¡ç¼“å­˜é•¿åº¦é‡ç®—
            self.set_high_performance(self.high_performance)

    def set_high_performance(self, enabled: bool):
        """åŠ¨æ€åˆ‡æ¢å›æº¯æ—¶é•¿ï¼šåŸºäºç›®æ ‡å°æ—¶æ•°å’ŒæŠ“å–é¢‘ç‡å¹³è¡¡å†…å­˜"""
        self.high_performance = enabled
        target_h = self.TARGET_HOURS_HP if enabled else self.TARGET_HOURS_LEGACY
        
        # ä¼˜å…ˆçº§ï¼šå¤–éƒ¨è®¾å®šçš„é¢‘ç‡ > è§‚æµ‹åˆ°çš„é¢‘ç‡ > 60s
        interval = self.expected_interval
        if interval <= 0:
            status = self.get_status()
            interval = status.get('avg_interval_sec', 60)
        
        if interval <= 0: interval = 60
        
        # 4h @ 60s = 240, 4h @ 120s = 120
        cache_len = int((target_h * 3600) / interval)
        cache_len = max(60, cache_len) # å…œåº•æœ€å° 60
        
        self.kline_cache.set_mode(max_len=cache_len)
        logger.info(f"ğŸš€ Mode: {'HP' if enabled else 'Legacy'} | Target: {target_h}h | Interval: {interval}s | Limit: {cache_len}K")

    def set_auto_switch(self, enabled: bool, threshold_mb: float = 500.0, node_limit: int = 1000000):
        """è®¾ç½®è‡ªåŠ¨åˆ‡æ¢è§„åˆ™"""
        self.auto_switch_enabled = enabled
        self.mem_threshold_mb = threshold_mb
        self.node_threshold = node_limit
        logger.info(f"âš™ï¸ Auto-Switch: enabled={enabled}, mem={threshold_mb}MB, nodes={node_limit}")

    def _maintenance_task(self):
        """
        åå°ç»´æŠ¤ä»»åŠ¡ï¼šæ¯ 10 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡å†…å­˜å’Œæ•°æ®é‡
        """
        while True:
            time.sleep(600)  # 10 minutes
            try:
                status = self.get_status()
                mem_mb = status.get('memory_usage_mb', 0)
                total_nodes = status.get('total_nodes', 0)
                
                # è‡ªåŠ¨é™çº§é€»è¾‘ (å†…å­˜è¶…é™ æˆ– èŠ‚ç‚¹æ•°è¶…é™)
                reason = ""
                if self.auto_switch_enabled and self.high_performance:
                    if mem_mb > self.mem_threshold_mb:
                        reason = f"Memory High ({mem_mb:.1f}MB)"
                    elif total_nodes > self.node_threshold:
                        reason = f"Nodes High ({total_nodes})"
                    
                    if reason:
                        logger.warning(f"âš ï¸ {reason}. Triggering Auto-Downgrade to Legacy Mode...")
                        self.set_high_performance(False)
                
                # Perf Mode info for logging
                is_hp = status.get('high_performance_mode', True)
                perf_str = "é«˜æ€§èƒ½ (å…¨å¤© 240m)" if is_hp else "æè‡´çœå†…å­˜ (æœ€è¿‘ 60m)"
                auto_str = "ON" if status.get('auto_switch') else "OFF"
                
                logger.info(f"ğŸ”§ [Maintenance] Mem: {status.get('memory_usage')} | "
                            f"Klines: {status.get('klines_cached')} | "
                            f"Updates: {status.get('update_count')}")
                
                # æ¯å°æ—¶æ›´æ–°ä¸€æ¬¡æ¿å—æŒç»­æ€§ç¼“å­˜
                if time.time() - self.last_db_check > 3600:
                    self._update_sector_cache()
                    self.last_db_check = time.time()
                
            except Exception as e:
                logger.error(f"Maintenance task error: {e}")

    def _update_sector_cache(self):
        """æ›´æ–°æ¿å—æŒç»­æ€§å¾—åˆ†"""
        from datetime import datetime, timedelta
        if not os.path.exists(self.db_path):
            return
        
        try:
            conn = sqlite3.connect(self.db_path)
            # è·å–æœ€è¿‘ 5 å¤©çš„æ•°æ®ï¼Œè®¡ç®—å‡ºç°é¢‘æ¬¡
            five_days_ago = (datetime.now() - timedelta(days=5)).strftime('%Y%m%d')
            query = f"""
                SELECT concept_name, COUNT(*) as freq 
                FROM concept_data 
                WHERE date >= '{five_days_ago}'
                GROUP BY concept_name 
                HAVING freq >= 2
                ORDER BY freq DESC
            """
            df_sec = pd.read_sql(query, conn)
            conn.close()
            
            if not df_sec.empty:
                # å°†é¢‘æ¬¡æ˜ å°„ä¸ºå¾—åˆ† (2æ¬¡: 0.5, 3æ¬¡: 0.8, 4+: 1.0)
                self.sector_cache = {}
                for row in df_sec.itertuples():
                    score = min(row.freq * 0.25, 1.0)
                    self.sector_cache[row.concept_name] = score
                logger.info(f"ğŸ“Š Sector persistence cache updated: {len(self.sector_cache)} sectors.")
        except Exception as e:
            logger.error(f"Error updating sector cache: {e}")

    def get_sector_score(self, sector_name: str) -> float:
        """è·å–æ¿å—æŒç»­æ€§å¾—åˆ†"""
        if not sector_name: return 0.0
        # æ¨¡ç³ŠåŒ¹é…æˆ–ç²¾ç¡®åŒ¹é…
        score = self.sector_cache.get(sector_name, 0.0)
        if score == 0:
            # ç®€å•å°è¯•åŒ…å«åŒ¹é…
            for name, s in self.sector_cache.items():
                if name in sector_name or sector_name in name:
                    return s
        return score

    def _scraper_task(self):
        """
        åå°æŠ“å–ä»»åŠ¡ï¼šå®šæœŸæŠ“å– 55188 æ•°æ®
        ä»…åœ¨äº¤æ˜“æ—¶æ®µè¿è¡Œï¼Œé‡åˆ°å°ç¦è¿¹è±¡è‡ªåŠ¨â€œç¿»å€å»¶è¿Ÿâ€ (Exponential Backoff)
        """
        while True:
            try:
                is_trading = cct.get_work_time_duration()
                now = time.time()
                
                # é€»è¾‘ï¼šç¨‹åºå¯åŠ¨æ—¶å¼ºåˆ¶æ‰§è¡Œç¬¬ä¸€æ¬¡æŠ“å–ï¼ˆlast_ext_update_ts == 0ï¼‰
                # ä¹‹åä»…åœ¨äº¤æ˜“æ—¶æ®µï¼ˆis_tradingï¼‰æŒ‰é—´éš”ï¼ˆcurrent_scraper_waitï¼‰æŠ“å–
                do_fetch = False
                if self.last_ext_update_ts == 0:
                    do_fetch = True
                elif is_trading:
                    delta = now - self.last_ext_update_ts
                    if delta >= self.current_scraper_wait:
                        do_fetch = True

                if do_fetch:
                    logger.info(f"ğŸ•¸ï¸ Fetching 55188 external data (init={self.last_ext_update_ts == 0}, wait={self.current_scraper_wait}s)...")
                    df_ext = self.scraper_55188.get_combined_data()
                    
                    if not df_ext.empty:
                        self.ext_data_55188 = df_ext
                        self.last_ext_update_ts = now
                        # æˆåŠŸåæ¢å¤é»˜è®¤å»¶è¿Ÿ
                        if self.current_scraper_wait != self.scraper_interval:
                            logger.info(f"âœ… Fetch success. Resetting scraper interval to {self.scraper_interval}s.")
                        self.current_scraper_wait = self.scraper_interval
                    else:
                        # å¤±è´¥æˆ–è¢«å°ç¦è¿¹è±¡ (è¿”å›ç©º) -> Double the wait
                        self.current_scraper_wait = min(self.current_scraper_wait * 2, self.max_scraper_wait)
                        # å¦‚æœæ˜¯åˆå§‹åŒ–å¤±è´¥ï¼Œä¹Ÿæ ‡è®°ä¸€ä¸‹ï¼Œé˜²æ­¢æ­»å¾ªç¯åœ¨è¿™ä¸ª if å—ï¼ˆè™½ç„¶ sleep 10s ä¼šç¼“è§£ï¼‰
                        if self.last_ext_update_ts == 0:
                            self.last_ext_update_ts = now - (self.current_scraper_wait / 2)
                        else:
                            self.last_ext_update_ts = now
                        logger.warning(f"âš ï¸ Fetch failed/Empty result. Doubling wait to {self.current_scraper_wait}s.")
                
            except Exception as e:
                # å¼‚å¸¸ä¹Ÿè§¦å‘ Backoff
                self.current_scraper_wait = min(self.current_scraper_wait * 2, self.max_scraper_wait)
                self.last_ext_update_ts = time.time()
                logger.error(f"Scraper task error: {e}. Backoff delay: {self.current_scraper_wait}s.")
            
            time.sleep(10) # ç»´æŒå¿ƒè·³æ£€æŸ¥é¢‘ç‡
        
    def update_batch(self, df: pd.DataFrame):
        """
        æ¥æ”¶æ¥è‡ª fetch_and_process çš„ DataFrame å¿«ç…§
        """
        if self.paused:
            return
            
        try:
            if df.empty: return

            # Fix: Ensure 'code' exists as a column (often in index)
            if 'code' not in df.columns:
                df = df.copy()
                df['code'] = df.index

            t0 = time.time()
            if self.last_batch_clock > 0:
                self.batch_intervals.append(t0 - self.last_batch_clock)
            self.last_batch_clock = t0
            
            rows_count = len(df)
            self.update_count += 1
            self.total_rows_processed += rows_count
            
            # 1. æ·±åº¦æƒ…ç»ªè®¡ç®— (Vectorized)
            if 'percent' in df.columns:
                # åŸºç¡€åˆ†ï¼š50 + æ¶¨å¹… * 3 (10% -> 80åˆ†, -10% -> 20åˆ†)
                base_score = 50 + (df['percent'] * 3)
                
                # é‡èƒ½åŠ æƒ (å‡è®¾ ratio ä¸ºé‡æ¯”, å¦‚æœæ²¡æœ‰åˆ™é»˜è®¤ä¸º 1)
                vol_ratio = df['ratio'] if 'ratio' in df.columns else pd.Series(1.0, index=df.index)
                
                # åŠ¨é‡ä¿®æ­£ï¼šé‡æ¯” > 1.5 ä¸”åŒå‘ï¼ŒåŠ å¤§æƒ…ç»ªæ³¢åŠ¨
                momentum = (vol_ratio - 1.0).clip(lower=0) * df['percent'] * 0.5
                
                final_score = base_score + momentum
                
                # ç‰¹æ®ŠçŠ¶æ€ä¿®æ­£
                # ææ…Œç›˜ï¼šè·Œå¹… > 7% ä¸”æ”¾é‡ -> æä½åˆ†
                # æŠ¢ç­¹ï¼šæ¶¨å¹… > 7% ä¸”æ”¾é‡ -> æé«˜åˆ†
                mask_panic = (df['percent'] < -7) & (vol_ratio > 1.5)
                mask_mania = (df['percent'] > 7) & (vol_ratio > 1.5)
                
                final_score.loc[mask_panic] -= 15
                final_score.loc[mask_mania] += 15
                
                # é™åˆ¶åœ¨ 0-100
                final_score = final_score.clip(0, 100)
                
                # Check for existing emotion score in DF (from upstream)
                if 'scan_score_emotion' in df.columns:
                    self.emotion_tracker.scores = df.set_index('code')['scan_score_emotion'].to_dict()
                else:
                    self.emotion_tracker.scores = dict(zip(df['code'], final_score))

            # Update global last update timestamp
            # 2. æ›´æ–° KLine (ä»…æ›´æ–°è®¢é˜…æˆ–æ´»è·ƒè‚¡) - Vectorized & Batch Optimized
            if 'trade' in df.columns:
                active_stocks = set(self.subscribers.keys()) | set(self.kline_cache.cache.keys())
                self.kline_cache.batch_update(df, active_stocks)
            
            # Record Speed
            t1 = time.time()
            duration = t1 - t0
            if duration > 0:
                self.batch_rates_dq.append(rows_count / duration)
            self.last_batch_time = t1
            
        except Exception as e:
            logger.error(f"DataPublisher update_batch error: {e}")

    def subscribe(self, code: str, callback: Callable):
        self.subscribers[code].append(callback)

    def get_minute_klines(self, code: str, n: int = 60):
        return self.kline_cache.get_klines(code, n)

    def get_emotion_score(self, code: str):
        return self.emotion_tracker.get_score(code)

    def get_v_shape_signal(self, code: str, window: int = 30) -> bool:
        """è·å–ä¸ªè‚¡æ˜¯å¦æœ‰ V å‹åè½¬ä¿¡å·"""
        return self.kline_cache.detect_v_shape(code, window)

    def get_55188_data(self, code: Optional[str] = None) -> Union[dict, dict[str, Any]]:
        """è·å–æŒ‡å®šçš„ 55188 å¤–éƒ¨æ•°æ® (äººæ°”ã€ä¸»åŠ›æ’åã€é¢˜æã€æ¿å—å¾—åˆ†ç­‰)"""
        if self.ext_data_55188.empty:
            return {}
        
        # å¦‚æœä¸ä¼  codeï¼Œè¿”å›å…¨é‡æ•°æ®å¿«ç…§æ±‡æ€»
        if code is None:
            return {
                'df': self.ext_data_55188.copy(),
                'last_update': cct.get_unixtime_to_time(self.last_ext_update_ts)
            }
            
        # ç»Ÿä¸€æŒ‰å­—ç¬¦ä¸²ç´¢å¼•å¤„ç†
        code_str = str(code).zfill(6)
        # å¦‚æœ code ä¸åœ¨ç´¢å¼•ä½†åœ¨åˆ—ä¸­ï¼Œé‡æ–°è®¾ä¸ºç´¢å¼•
        if 'code' in self.ext_data_55188.columns and self.ext_data_55188.index.name != 'code':
            self.ext_data_55188 = self.ext_data_55188.set_index('code')
            
        if code_str in self.ext_data_55188.index:
            data = self.ext_data_55188.loc[code_str].to_dict()
            # æ³¨å…¥æ¿å—æŒç»­æ€§å¾—åˆ†
            theme_name = data.get('theme_name', '')
            data['sector_score'] = self.get_sector_score(theme_name)
            return data
        return {}

    def stress_test(self, num_stocks=4000, n_klines=240):
        """å†…å­˜å‹åŠ›æµ‹è¯•"""
        import sys
        print(f"Starting Stress Test: {num_stocks} stocks, {n_klines} klines each...")
        dummy_data = {
            'time': 1700000000, 'open': 10.0, 'high': 11.0, 'low': 9.0, 'close': 10.5, 'volume': 1000
        }
        for i in range(num_stocks):
            code = f"600{i:03d}"
            for _ in range(n_klines):
                self.kline_cache.cache[code].append(dummy_data)
        
        # ä¼°ç®—å†…å­˜
        # è¿™æ˜¯ä¸€ä¸ªç²—ç•¥ä¼°ç®—
        # å®é™…å¯¹è±¡å¼€é”€è¾ƒå¤§
        print("Stress Test Populated. Check Task Manager for memory usage.")
        
    def get_status(self) -> dict[str, Any]:
        """
        è·å–æœåŠ¡è¿è¡ŒçŠ¶æ€ç›‘æ§æŒ‡æ ‡
        """
        try:
            # Memory Usage
            mem_info = "N/A"
            if psutil:
                process = psutil.Process(os.getpid())
                mem_bytes = process.memory_info().rss
                mem_info = f"{mem_bytes / 1024 / 1024:.2f} MB"
            
            # Speed
            avg_speed = 0
            if self.batch_rates_dq:
                avg_speed = sum(self.batch_rates_dq) / len(self.batch_rates_dq)
            
            try:
                cpu_usage = process.cpu_percent(interval=None)
            except:
                cpu_usage = 0.0
                
            uptime = time.time() - self.start_time
            
            total_nodes = sum(len(d) for d in self.kline_cache.cache.values())
            avg_nodes = total_nodes / len(self.kline_cache.cache) if self.kline_cache.cache else 0
            
            # Estimate History Coverage
            # ä¼˜å…ˆçº§ï¼šç›´æ¥ä½¿ç”¨é¢„æœŸçš„æŠ“å–é¢‘ç‡ï¼Œå¦‚æœæ²¡æœ‰æŠ“å–è¿‡æ•°æ®ï¼Œåˆ™ä½¿ç”¨ expected_interval
            # åªæœ‰åœ¨é¢„æœŸé¢‘ç‡å’Œè§‚æµ‹é¢‘ç‡éƒ½ç¼ºå¤±æ—¶æ‰é»˜è®¤ 60s
            avg_interval = self.expected_interval
            if self.batch_intervals:
                avg_interval = sum(self.batch_intervals) / len(self.batch_intervals)
            
            if avg_interval <= 0: avg_interval = 60
            
            history_sec = avg_nodes * avg_interval
            
            return {
                "klines_cached": len(self.kline_cache.cache),
                "total_nodes": total_nodes,
                "avg_nodes_per_stock": avg_nodes,
                "avg_interval_sec": int(avg_interval),
                "expected_interval": self.expected_interval,
                "history_coverage_minutes": int(history_sec / 60),
                "subscribers": sum(len(v) for v in self.subscribers.values()),
                "emotions_tracked": len(self.emotion_tracker.scores),
                "paused": self.paused,
                "high_performance_mode": self.high_performance,
                "target_hours": self.TARGET_HOURS_HP if self.high_performance else self.TARGET_HOURS_LEGACY,
                "auto_switch": self.auto_switch_enabled,
                "mem_threshold": self.mem_threshold_mb,
                "node_threshold": self.node_threshold,
                "node_capacity_pct": (total_nodes / self.node_threshold * 100) if self.node_threshold else 0,
                "cpu_usage": cpu_usage,
                "max_batch_time_ms": int(self.max_batch_time * 1000),
                "last_batch_time_ms": int(self.last_batch_time * 1000),
                "cache_history_limit": self.kline_cache.max_len,
                "last_update": self.kline_cache.last_update_ts.get("global", 0),
                "server_time": time.time(),
                "uptime_seconds": int(uptime),
                "memory_usage": mem_info,
                "memory_usage_mb": float(mem_info.split()[0]) if mem_info != "N/A" else 0,
                "total_rows_processed": self.total_rows_processed,
                "update_count": self.update_count,
                "processing_speed_row_per_sec": int(avg_speed),
                "pid": os.getpid()
            }
        except Exception as e:
            logger.error(f"get_status error: {e}")
            return {"error": str(e)}

if __name__ == "__main__":
    # ğŸ§ª Standalone Test Functionality
    import sys
    print("ğŸš€ Starting Standalone RealtimeDataService Test...")
    
    # --- Configuration ---
    USE_HIGH_PERFORMANCE = True # Toggle this to False for 60m History Mode
    
    # 1. Initialize Service
    dp = DataPublisher(high_performance=USE_HIGH_PERFORMANCE)
    stock_count = 5000
    
    # 2. Create Dummy Data
    def create_dummy_data(n=5):
        codes = [f"600{i:03d}" for i in range(n)]
        data = {
            "code": codes,
            "name": [f"Stock_{c}" for c in codes],
            "trade": [10.0 + i for i in range(n)],
            "percent": [1.5 + i*0.1 for i in range(n)],
            "high": [10.5 + i for i in range(n)],
            "low": [9.8 + i for i in range(n)],
            "vol": [1000 + i*100 for i in range(n)],
            "amount": [10000 + i*1000 for i in range(n)],
            "time": time.strftime("%H:%M:%S")
        }
        return pd.DataFrame(data)

    # 3. Test normal operation
    dummy_df = create_dummy_data(n=stock_count) # Using 10 rows for better visibility
    print(f"\n[Test 1] Normal Update (Rows: {len(dummy_df)} shape:{dummy_df.shape})...")
    print("Dummy Data Head:")
    print(dummy_df.head())
    
    dp.update_batch(dummy_df)
    status = dp.get_status()
    print(f"âœ… Updates: {status.get('update_count')}, Memory: {status.get('memory_usage')}")

    # 4. Test Pause
    print("\n[Test 2] Pausing Service...")
    dp.set_paused(True)
    dp.update_batch(dummy_df) # This should be ignored
    status_paused = dp.get_status()
    if status_paused.get('update_count') == status.get('update_count'):
        print("âœ… Pause Successful: Update ignored.")
    else:
        print(f"âŒ Pause Failed: Update count increased to {status_paused.get('update_count')}")

    # 5. Test Reset
    print("\n[Test 3] Resetting State...")
    dp.reset_state()
    status_reset = dp.get_status()
    if status_reset.get('update_count') == 0 and status_reset.get('klines_cached') == 0:
        print("âœ… Reset Successful: Counters and Cache cleared.")
    else:
        print(f"âŒ Reset Failed: {status_reset}")

    # 6. Resume and Update
    print("\n[Test 4] Resuming and Updating...")
    dp.set_paused(False)
    dp.update_batch(dummy_df)
    status_final = dp.get_status()
    print(f"âœ… Final Updates: {status_final.get('update_count')}")
    
    # 7. Simulation: 4 Hours of Trading Data (30s iterations)
    # 4 hours = 240 minutes = 480 iterations (at 30s interval)
    print("\n[Test 5] Simulating 4-Hour Trading Session (480 batches of 1000 stocks)...")
    print("This will correctly fill the 240-minute cache per stock.")
    
    # Subscribe stocks (increased to 1000 to see real growth)
    for i in range(stock_count):
        code = f"600{i:03d}"
        dp.subscribe(code, lambda x: None)
        
    start_sim = time.time()
    total_batches = 480
    base_ts = int(time.time()) - (total_batches * 30) # Start 4 hours ago
    
    for i in range(total_batches):
        sim_df = create_dummy_data(n=stock_count)
        # Mock timestamp incrementing by 30s each batch
        current_sim_ts = base_ts + (i * 30)
        sim_df['timestamp'] = current_sim_ts
        
        # Timing the individual batch
        batch_start = time.time()
        dp.update_batch(sim_df)
        batch_end = time.time()
        batch_dur = batch_end - batch_start
        
        if (i + 1) % 100 == 0:
            current_status = dp.get_status()
            print(f"  > Batch {i+1}/{total_batches} processed. "
                  f"Last Batch: {batch_dur*1000:.2f}ms | "
                  f"Mem: {current_status.get('memory_usage')} | "
                  f"Klines: {current_status.get('klines_cached')}")
            
    end_sim = time.time()
    final_status = dp.get_status()
    klines_count = final_status.get('klines_cached', 0) or 0
    total_nodes = final_status.get('total_nodes', 0) or 0
    print(f"\nâœ… Simulation Complete in {end_sim - start_sim:.2f} seconds.")
    
    current_mem = final_status.get('memory_usage_mb', 0) or 0
    mem_used_kb = (float(current_mem) - 55.0) * 1024 # KB above base
    per_node = (mem_used_kb * 1024 / total_nodes) if total_nodes > 0 else 0
    
    print(f"ğŸ“Š Final Stats ({stock_count} Stocks * 240 Mins):")
    print(f"   - Total Updates: {final_status.get('update_count')}")
    print(f"   - Memory Usage: {final_status.get('memory_usage')}")
    print(f"   - KLines Cached (Stocks): {klines_count}")
    print(f"   - Total Nodes across all deques: {total_nodes}")
    print(f"   - Avg Nodes per Stock: {final_status.get('avg_nodes_per_stock', 0):.1f}")
    print(f"   - Est. Incremental Memory per Node: {per_node:.1f} bytes")
    
    print("\nâœ¨ Test Sequence Completed.")
